:language_code: pt_br

========
Projeto4
========

:Autor: saitama
:Data: 05/06/2016

Tema: Reconhecimento de imagens semelhantes

Integrantes: 

Gustavo Ciotto Pinton

Pedro Mariano de Sousa Bezerra

Joseph Lorenzo

==============
Introdução
==============

O objetivo do projeto é a confecção de um CBIR (Content-based image retrieval): utilizar um algoritmo de clusterização, mais precisamente o K-means, para classificar imagens que possuem atributos semelhantes
em um mesmo cluster, a fim de buscar imagens parecidas a partir de uma imagem fornecida pelo usuário e que não foi utilizada no 'treinamento'.

Utilizaremos uma base de 25000 imagens retiradas do Flickr para realizar o treinamento, e a versão do algoritmo K-means utilizada será a fornecida pela biblioteca Scipy.
O treinamento dos dados será dividido em duas etapas. Na primeira, executaremos o K-means sobre as 25000 imagens utilizando os chamados atributos de textura de Tamura para classificação
de imagens: o nível de contraste, o grau de direcionalidade e a granulação (coarseness). Em adição, propomos também o uso dos filtros de Gabor, cujo propósito também é de adquirir métricas da textura das imagens. Posteriormente, na segunda etapa, executaremos novamente o K-means para cada cluster de imagens
obtido na primeira etapa, desta vez utilizando como atributos o histograma de cores em nível de cinza, a fim de obter um segundo nível de clusterização. Faremos isso para tornar mais precisa a classificação
das imagens e melhorar os resultados finais. A escolha do uso de imagens em tons de cinza foi tomada com o intuito de simplicar e agilizar o processamento, tendo em vista que possuimos uma largo banco de dados e que os algoritmos apresentam complexidades que variam exponencialmente com o número de amostras a serem classificadas.

O processo de obtenção de imagens semelhantes a uma imagem dada também ocorre em duas etapas. Inicialmente, calcula-se o cluster de primeiro nível ao qual pertence a imagem fornecida,
levando-se em conta apenas os atributos de Tamura e de Gabor. Em seguida, calcula-se o cluster de segundo nível levando-se em conta o histograma em tons de cinza. Finalmente, dentre as imagens que estão no mesmo
cluster que a imagem fornecida, apresentamos como resultado aquelas que apresentarem menor diferença entre os valores dos seus atributos e os da imagem de entrada.

==============================
Atributos de Textura de Tamura
==============================

A maioria dos sistemas CBIR atualmente usam os chamados atributos de Tamura, descritos a seguir, obtidos a partir de uma imagem em escala de cinza:

- Contraste: mede o quanto os níveis de cinza variam em uma imagem e o quão concentrada é a sua distribuição em preto ou branco. Os momentos centralizados de segunda ordem (variância -  σ²) e 
  normalizado de quarta ordem (kurtosis - α4) do histograma h de nível de cinza são utilizados para definir o contraste.   


.. equation:: latex

    F_{con} = \frac{\sigma}{\alpha_4^n},\>onde:\>\alpha_4 = \frac{\mu_4}{\sigma^4};\>\sigma^2 = \sum_{q=0}^{q_{max}} (q-m)^2 \frac{h(q)}{N};\>\mu_{4} = \sum_{q=0}^{q_{max}} (q-m)^4 \frac{h(q)}{N}


Onde m é a média do nível de cinza q dos pixels da imagem, μ4 é o momento centralizado de quarta ordem, N é o número de pixels e o valor de n é tipicamente 0,25.

- Grau de direcionalidade: medida da distruibuição da frequência de bordas locais orientadas de acordo com seus ângulos direcionais. A intensidade da borda e(x,y) e o ângulo direcional α(x,y) são 
  calculados com o auxílio do filtro de Sobel para detecção de bordas ao longo das direções horizontal e vertical, usando respectivamente os seguintes operadores de janela 3x3, que resultam nas imagens
  Δx e Δy:
    
.. code:: python
    :show_code:   no
   
    image = adread('/awmedia/www/media/Attachments/courseEA9791s2016/Projeto4/janelas.png')
    adshow(image)

.. equation:: latex

    e(x,y) = 0.5(|\Delta_x(x,y)| + |\Delta_y(x,y)|)
    
.. equation:: latex  

    \alpha(x,y) = \tan^{-1}\left(\frac{\Delta_y(x,y)}{\Delta_x(x,y)}\right)
    
Em seguida, obtém-se o histograma direcional Hdir(a) de valores 'a' de direção quantizados, contando-se o número de ocorrências de pixels de borda com os ângulos direcionais correspondentes e cuja intensidade 
da borda e(x,y) seja superior a um valor predefinido. O histograma é relativamente uniforme para imagens sem forte orientação e apresenta picos para images altamente direcionais. O grau de direcionalidade Fdir
está relacionado com a agudeza dos picos:

.. equation:: latex  

    F_{dir} = 1 - rn_{peaks}\sum_{p=1}^{n_{peaks}}\sum_{a\in w_p}(a-a_p)^2H_{dir}(a)

Onde n_peaks é o número de picos do histograma, a_p é a posição do p-ésimo pico, w_p é o conjunto de ângulos atribuídos ao p-ésimo pico (isto é, os ângulos compreendidos entre os vales em torno de um pico), r é
um fator de normalização relacionado com os níveis quantizados dos ângulos 'a', e 'a' é o ângulo direcional quantizado (ciclicamente em módulo 180º). 
A seguir, apresentamos um exemplo de imagem para um valor alto do grau de direcionalidade e outro exemplo para um valor baixo.

.. code:: python
    :show_code:   no
    
    img1 = adread('/awmedia3/ea979_1S2016/CBIR_Kmeans/im1127.jpg')
    adshow(img1,'Figura 1: imagem altamente direcional (grau = 0.98643)')
    hist1 = adread('/awmedia/www/media/Attachments/courseEA9791s2016/Projeto4/Histograma mto direcional.png')
    adshow(hist1,'Figura 2: Histograma da imagem da figura 1')
    
    img2 = adread('/awmedia3/ea979_1S2016/CBIR_Kmeans/im1224.jpg')
    adshow(img2,'Figura 3: imagem fracamente direcional (grau = 0.00076)')
    hist2 = adread('/awmedia/www/media/Attachments/courseEA9791s2016/Projeto4/Histograma pouco direcional.png')
    adshow(hist2,'Figura 4: histograma da imagem da figura 3')

O histograma da imagem da figura 1 mostra que as bordas da imagem se concentram na direção vertical. Já o histograma da imagem da figura 3 é relativamente uniforme, pois a imagem não é fortemente orientada.

- Granulação (coarseness): relaciona-se com as distâncias entre variações espaciais notáveis dos níveis de cinza da imagem, isto é, implicitamente, ao tamanho dos elementos primitivos (texels) que formam a textura. O cálculo é feito a partir das diferenças entre as imagens obtidas após a aplicação de filtros da média com diferentes tamanhos de janela.

O módulo escrito abaixo fornece as funções que implementam o cálculo destas features, de acordo com a imagem recebida como parâmetro.

Tamura Library
==============

.. code:: python
    :show_code:   yes
    
    import numpy as np
    from scipy import signal
    
    import os
    
    threshold = 25 # Threshold for Sobel filter
    neigh = 20 # Neighborhood size for finding peaks in directional histogram
    
    #Convert RGB image to Gray Scale
    def rgb2gray(rgb):
        return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])
    
    #Calculate the pixels mean value
    def mean(hist,nPixels):
        m = 0.0
        for i in np.arange(256):
            m += i*hist[i]
        return m/nPixels
    
    #Calculate the n-order central moment
    def moment(n,hist,mean,nPixels):
        v = 0.0
        for i in np.arange(256):
            v += (i - mean)**n *hist[i]
        return v/nPixels
    
    #Calculate contrast of the gray image
    def contrast(gray):
        n = 0.25
        nPixels = gray.size
        hist, bins = np.histogram(gray,256)
        m = mean(hist,nPixels)
        sigma2 = moment(2,hist,m,nPixels)
        sigma = np.sqrt(sigma2)
        kurtosis = moment(4,hist,m,nPixels)/(sigma2**2)
        return sigma / (kurtosis**n)
    
    #Applies Sobel filter to the gray image with the specified threshold for edges strength
    def sobelFilter(gray,threshold):
        sobelX = np.array([[-1,0,1],[-1,0,1],[-1,0,1]])
        sobelY = np.array([[1,1,1],[0,0,0],[-1,-1,-1]])
    
        gradX = signal.convolve2d(gray,sobelX,boundary='wrap',mode='same')
        gradY = signal.convolve2d(gray,sobelY,boundary='wrap',mode='same')
        edgeStrength = 0.5*(np.abs(gradX) + np.abs(gradY))
        edgeStrength *= 255/np.max(edgeStrength)
    
        #Applies threshold transformation to edges strength
        T = np.arange(256)
        T[T < threshold] = 0
        edgeStrength = edgeStrength.astype(np.uint8)
        edgeStrength = T[edgeStrength]
    
        directAngle = np.zeros(gray.shape)
        rows, columns = gray.shape
        for i in range(rows):
            for j in range(columns):
                if(gradX[i,j] != 0):
                    directAngle[i,j] = np.arctan(gradY[i,j]/gradX[i,j])
                else:
                    directAngle[i,j] = np.pi/2
    
        directAngle[directAngle < 0.0] += np.pi
        directAngle *= 180/np.pi
        directAngle = np.mod(directAngle.astype(np.uint8),180)
    
        return (edgeStrength,directAngle)
    
    #Calculate directional histogram
    def histDirect(directAngle,edgeStrength):
        nBins = 180
        hist = np.zeros(nBins)
        bins = np.arange(nBins+1)
        rows,columns = directAngle.shape
        for i in range(rows):
            for j in range(columns):
                if(edgeStrength[i,j] > 0):
                    hist[directAngle[i,j]] += 1
        return (hist,bins)
    
    #Find peaks of directional histogram
    def findPeaks(histD,neigh):
        nPeaks = 0
        less = np.zeros(neigh) 
        more = np.zeros(neigh)
        peaks = np.array([])
        j = 0
        while(j < 180): #Verify if j is the index of a peak
            found = True
            for i in range(neigh):
                less[i] = j-i-1 #indexes of elements to the left of j
                more[i] = j+i+1 #indexes of elements to the right of j
            less[less<0] += 180
            more[more>179] -= 180
            for i in np.arange(neigh-1):
                if(histD[less[i]] >= histD[j] or histD[j] <= histD[more[i]]):
                    found = False
                    break
            if(histD[less[neigh-1]] > histD[j] or histD[j] < histD[more[neigh-1]]):
                found = False
            if(found):
                peaks = np.insert(peaks,nPeaks,j)
                nPeaks += 1
                j += neigh - 1
    
            j += 1
    
        if(peaks.size == 0): # When no peaks couldn't be found, set the max value as a peak
            maxValue = np.where(histD == np.max(histD))
            peaks = np.array([maxValue[0][0]])
            
        return peaks
    
    #Calculate ranges of peaks
    def findRanges(peaks, hist):
        nPeaks = peaks.size
        if(nPeaks > 1):
            rangePeaks = np.zeros(nPeaks)
        else:
            rangePeaks = np.zeros(nPeaks+1)
    
        #Left limit for firstpeak
        if(nPeaks > 1):
            rangeSize = peaks[0] - peaks[nPeaks-1] + 180
            angles = np.arange(rangeSize-1) + peaks[nPeaks-1] + 1
            angles[angles>=180] -= 180
            minAng = peaks[nPeaks-1]
            for j in angles:
                if(hist[minAng] > hist[j]):
                    minAng = j
            rangePeaks[0] = minAng
        else:
            rangePeaks[0] = peaks[0]-90
            if(rangePeaks[0] < 0): rangePeaks[0] +=180
            rangePeaks[1] = peaks[0]+90
            if(rangePeaks[1] >=180): rangePeaks[1] -=180
    
        #Limits for other peaks    
        for i in np.arange(nPeaks-1)+1:
            rangeSize = peaks[i] - peaks[i-1]
            angles = np.arange(rangeSize-1) + peaks[i-1] + 1
            minAng = peaks[i-1]
            for j in angles:
                if(hist[minAng] > hist[j]):
                    minAng = j
            rangePeaks[i] = minAng
            
        return rangePeaks
    
    #Calculate degree of directionality of the gray image
    def degreeDirect(gray, threshold,neigh):
    
        sum = 0
    
        edgeStrength,directAngle = sobelFilter(gray,threshold)
        histD,bins = histDirect(directAngle,edgeStrength)
        peaks = findPeaks(histD,neigh)
        rangePeaks = findRanges(peaks,histD)
    
        nPeaks = peaks.size
        sumHist = np.sum(histD)
        r = 20.0/3
        quant = 179 # quantizing leves
        
        # First Peak
        rangeSize = rangePeaks[1] - rangePeaks[0]
        if(rangeSize <= 0):
            rangeSize += 180
            angles = np.arange(rangeSize) + rangePeaks[0]
            if(rangePeaks[0] > peaks[0]): diff = angles - 180
            else: diff = np.copy(angles)
            angles[angles >= 180] -= 180
        else:
            angles = np.arange(rangeSize) + rangePeaks[0]
            diff = angles
        for j in np.arange(rangeSize):
            sum += ((diff[j] - peaks[0])/quant)**2 * histD[angles[j]]/sumHist
            
        #Peaks in the middle
        for i in np.arange(nPeaks-2)+1:
            rangeSize = rangePeaks[i+1] - rangePeaks[i]
            angles = np.arange(rangeSize) + rangePeaks[i]
            for j in angles:
                sum += ((j - peaks[i])/quant)**2 * histD[j]/sumHist
    
        #Last peak
        rangeSize = rangePeaks[0] - rangePeaks[nPeaks-1]
        if(rangeSize < 0):
            rangeSize += 180
            angles = np.arange(rangeSize) + rangePeaks[nPeaks-1]
            diff = np.copy(angles)
            angles[angles >= 180] -= 180
        else:
            angles = np.arange(rangeSize) + rangePeaks[nPeaks-1]
            diff = angles
        for j in np.arange(rangeSize):
            sum += ((diff[j] - peaks[nPeaks-1])/quant)**2 * histD[angles[j]]/sumHist
    
        sum *= r*nPeaks
        return 1 - sum
    
    #### JOSEPH Codes
    
    def coarseness(gray):
        
        (rows, cols) = gray.shape
        
        f1 = np.ones((2,2))/(2**2)
        f2 = np.ones((4,4))/(2**4)
        f3 = np.ones((8,8))/(2**6)
        f4 = np.ones((16,16))/(2**8)
        f5 = np.ones((32,32))/(2**10)
        f6 = np.ones((64,64))/(2**12)
        
        kArr = []
        kArr.append(signal.convolve2d(gray,f1,boundary='symm',mode='same'))
        kArr.append(signal.convolve2d(gray,f2,boundary='symm',mode='same'))
        kArr.append(signal.convolve2d(gray,f3,boundary='symm',mode='same'))
        kArr.append(signal.convolve2d(gray,f4,boundary='symm',mode='same'))
        kArr.append(signal.convolve2d(gray,f5,boundary='symm',mode='same'))
        kArr.append(signal.convolve2d(gray,f6,boundary='symm',mode='same'))
        
        counter = 0        
        sm = 0
        ak1 = 0
        ak2 = 0
        
        kErr = []
        for k in range (0,6):
            kErr.append(np.zeros((rows,cols)))
        for k in range (0,6):
            for i in range (0,rows):
                for j in range(0,cols):
                    if (i - 1) >= 0:
                        ak1 = abs(kArr[k][i-1][j] - kArr[k][i][j])
                    if (i + 1) < rows:
                        ak2 = abs(kArr[k][i+1][j] - kArr[k][i][j])
                    if ak1 >= ak2:
                        sm += ak1
                    else:
                        sm += ak2
                    ak1 = 0
                    ak2 = 0
                    if (j - 1) >= 0:
                        ak1 = abs(kArr[k][i][j-1] - kArr[k][i][j])
                    if (j + 1) < cols:
                        ak2 = abs(kArr[k][i][j+1] - kArr[k][i][j])
                    if ak1 >= ak2:
                        sm += ak1
                    else:
                        sm += ak2
                    kErr[k][i][j] = sm
                    if sm > 0:
                        counter = counter + 1
                    sm = 0
                
        arrkMax = np.zeros((rows,cols))
        for i in range(0,rows):
            for j in range(0,cols):
                arrkMax[i][j] = 2
                eMax = kErr[0][i][j]
                for k in range (1,6):
                    if kErr[k][i][j] > eMax:
                        eMax = kErr[k][i][j]
                        arrkMax[i][j] = 2**(k+1)
        return np.mean(arrkMax)
        

==============================
Atributos de Textura de Gabor
==============================

Os filtros de Gabor também podem ser utilizados para a detecção de bordas, à medida que as frequências e orientações 
em tais filtros correspondem às mesmas características encontradas na visão humana. No caso, utilizamos 24 filtros, obtidos a partir da variação de ângulos e frequências na seguinte equação:

.. equation:: latex
  
    g(x,y;\lambda,\theta,\psi,\sigma,\gamma) = \exp\left(-\frac{x'^2+\gamma^2y'^2}{2\sigma^2}\right)\exp\left(i\left(2\pi\frac{x'}{\lambda}+\psi\right)\right)

As figuras abaixo representam alguns filtros obtidos a partir da equação acima. Destaca-se que a imagem constituída de tijolos (brick) responde muito bem aos filtros com theta = 0,
visto que possui linhas verticais muito definidas, contrariamente às imagens contendo grama (grass) e muro (wall), que apresentam, por sua vez, resultados muito semelhantes em relação a todos eles, à medida que não têm nenhum tipo de linearidade tanto na vertical como na horizontal.

.. code:: python

   g = adread('plot_gabor_1.png')
   adshow(g,'Figura 5: Exemplos de filtros de Gabor aplicadas a imagens distintas. Retirada de http://scikit-image.org/docs/dev/auto_examples/plot_gabor.html.')

O módulo escrito abaixo propõe uma implementação destes filtros com diversos valores de theta, sigma e de frequência. Ao todo, 24 filtros foram gerados, sendo que para cada um, duas features são recuperadas: a média e a variância da imagem filtrada.

Gabor Library
==============
    
.. code:: python    
    
    import numpy as np
    from scipy import ndimage as ndi
    
    def _sigma_prefactor(bandwidth):
        b = bandwidth
        # See http://www.cs.rug.nl/~imaging/simplecell.html
        return 1.0 / np.pi * np.sqrt(np.log(2) / 2.0) * \
            (2.0 ** b + 1) / (2.0 ** b - 1)
    
    
    def gabor_kernel(frequency, theta=0, bandwidth=1, sigma_x=None, sigma_y=None,
                     n_stds=3, offset=0):
        """Return complex 2D Gabor filter kernel.
    
        Gabor kernel is a Gaussian kernel modulated by a complex harmonic function.
        Harmonic function consists of an imaginary sine function and a real
        cosine function. Spatial frequency is inversely proportional to the
        wavelength of the harmonic and to the standard deviation of a Gaussian
        kernel. The bandwidth is also inversely proportional to the standard
        deviation.
    
        Parameters
        ----------
        frequency : float
            Spatial frequency of the harmonic function. Specified in pixels.
        theta : float, optional
            Orientation in radians. If 0, the harmonic is in the x-direction.
        bandwidth : float, optional
            The bandwidth captured by the filter. For fixed bandwidth, `sigma_x`
            and `sigma_y` will decrease with increasing frequency. This value is
            ignored if `sigma_x` and `sigma_y` are set by the user.
        sigma_x, sigma_y : float, optional
            Standard deviation in x- and y-directions. These directions apply to
            the kernel *before* rotation. If `theta = pi/2`, then the kernel is
            rotated 90 degrees so that `sigma_x` controls the *vertical* direction.
        n_stds : scalar, optional
            The linear size of the kernel is n_stds (3 by default) standard
            deviations
        offset : float, optional
            Phase offset of harmonic function in radians.
    
        Returns
        -------
        g : complex array
            Complex filter kernel.
    
        References
        ----------
        .. [2] http://en.wikipedia.org/wiki/Gabor_filter
        .. [3] http://mplab.ucsd.edu/tutorials/gabor.pdf
    
        Examples
        --------
        >>> from skimage.filters import gabor_kernel
        >>> from skimage import io
        >>> from matplotlib import pyplot as plt  # doctest: +SKIP
    
        >>> gk = gabor_kernel(frequency=0.2)
        >>> plt.figure()        # doctest: +SKIP
        >>> io.imshow(gk.real)  # doctest: +SKIP
        >>> io.show()           # doctest: +SKIP
    
        >>> # more ripples (equivalent to increasing the size of the
        >>> # Gaussian spread)
        >>> gk = gabor_kernel(frequency=0.2, bandwidth=0.1)
        >>> plt.figure()        # doctest: +SKIP
        >>> io.imshow(gk.real)  # doctest: +SKIP
        >>> io.show()           # doctest: +SKIP
        """
        if sigma_x is None:
            sigma_x = _sigma_prefactor(bandwidth) / frequency
        if sigma_y is None:
            sigma_y = _sigma_prefactor(bandwidth) / frequency
    
        x0 = np.ceil(max(np.abs(n_stds * sigma_x * np.cos(theta)),
                         np.abs(n_stds * sigma_y * np.sin(theta)), 1))
        y0 = np.ceil(max(np.abs(n_stds * sigma_y * np.cos(theta)),
                         np.abs(n_stds * sigma_x * np.sin(theta)), 1))
        y, x = np.mgrid[-y0:y0 + 1, -x0:x0 + 1]
    
        rotx = x * np.cos(theta) + y * np.sin(theta)
        roty = -x * np.sin(theta) + y * np.cos(theta)
    
        g = np.zeros(y.shape, dtype=np.complex)
        g[:] = np.exp(-0.5 * (rotx ** 2 / sigma_x ** 2 + roty ** 2 / sigma_y ** 2))
        g /= 2 * np.pi * sigma_x * sigma_y
        g *= np.exp(1j * (2 * np.pi * frequency * rotx + offset))
    
        return g

    #############################################################
    # References:
    # http://scikit-image.org/docs/dev/auto_examples/plot_gabor.html

    from skimage import data
    from skimage.util import img_as_float
    # from skimage.filters import gabor_kernel
    from scipy import ndimage as ndi
    import numpy as np
    
    
    def garbor_features(image, kernels):
        feats = np.zeros((len(kernels), 2), dtype=np.double)
        for k, kernel in enumerate(kernels):
            filtered = ndi.convolve(image, kernel, mode='wrap')
            feats[k, 0] = filtered.mean()
            feats[k, 1] = filtered.var()
        return feats
    
    
    
    # prepare filter bank kernels
    kernels = []
    for theta in range(4):
        theta = theta / 4. * np.pi
        for sigma in (1, 3):
            for frequency in (0.05, 0.25):
                kernel = np.real(gabor_kernel(frequency, theta=theta,
                                              sigma_x=sigma, sigma_y=sigma))
                kernels.append(kernel)

================================================
Pré-processamento das imagens do banco de dados
================================================

O módulo abaixo realiza o pré-processamento das 25000 imagens da nossa base. A cada processamento de uma imagem, um arquivo é gerado para cada uma de suas features, a fim de que, caso o processamento seja interrompido, não seja necessário iniciar o procedimento do inicío. 
Além disso, as flags useGreyScale, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality e useGarbor permitem a escolha de quais features serão utilizadas. As variáveis kCentroids_features, cIter_features, kCentroids e cIter especificam o número de centróides e iterações usadas para, respectivamente, o 1o e 2o nível da clusterização.
Para o primeiro nível, utilizamos 200 centróides e 100 iterações, enquanto que, para o segundo, usamos 20 centróides e 100 iterações, igualmente. Estimamos que, se em uma situação hipotética em que cada centróide receba a mesma quantidade de imagens, cada um do segundo nível será composto por aproximadamente 6 imagens, já que 25000 / (200 * 20) = 6.25. 

A fim de buscarmos rapidamente os centróides sem que todo o processamento seja refeito a cada nova procura, alguns arquivos são gerados e salvos após cada nível:

- Codebook: possui as features de cada centroide.
- Standart deviation: o algoritmo do módulo scipy necessita que a matriz de observações (amostras) contenha colunas de variância unitária. Assim, chamamos a função whitten, responsável por normalizá-las, e armazenamos a constante utilizada para cada uma delas. Dessa forma, quando recebermos uma imagem desconhecida, saberemos o fator que deve ser aplicado igualmente a cada uma de sua colunas.
- Codes: matriz que associa cada imagem do banco de dados ao centróide que lhe foi atribuído. Essa matriz será utilizada para resgatarmos as imagens mais semelhantes àquela fornecida pelo usuário.
- Vq_dist: matriz de erros entre cada imagem e seu centróide.

Além destas observações, comentários foram realizados diretamente no código abaixo. 

Módulo process_images.py
========================

.. code:: python
    
    '''
    # References
    http://people.kmi.open.ac.uk/stefan/www-pub/howarth-rueger-2004-civr-texture.pdf
    '''
    
    import numpy as np
    from scipy import misc
    import scipy.cluster as spy
    import os
    import time
    
    # Caminho para imagens
    images_path = "/awmedia3/ea979_1S2016/CBIR_Kmeans"
    
    n_images = 25000
    
    # Parametros utilizados pelo KMEANS
    kCentroids_features = 200
    cIter_features = 100
    
    kCentroids = 20
    cIter = 100
    
    # Escolha dos FEATURES
    useGreyScale = True
    useTamuraCoarseness = True
    useTamuraContrast = True
    useTamuraDirectionality = True
    useGarbor = True
    
    if useGreyScale:
        n_columns_histogram = 256    
    else: 
        n_columns_histogram = 768
    
    n_colums_features = 0
    n_kernels = 0
    
    failures_directionality = []
    failures_contrast = []
    failures_coarseness = []
    failures_garbor = []
    
    # Verifica se ja ha arquivo contendo os histogramas   
    histogram_hasBeenCalculated = os.path.isfile('%s/features/histograms.npy' % images_path)
    
    print 'Histograms file found? ', histogram_hasBeenCalculated
    
    # Se sim, carregue-o
    if histogram_hasBeenCalculated:
        learning_set = np.load('%s/features/histograms.npy' % images_path)
    else : learning_set = np.zeros((n_images, n_columns_histogram))
    
    # Verifica se ja ha arquivo contendo os features de coarnesess e, caso houver,
    # carregue-o
    if useTamuraCoarseness:
    
        n_colums_features = n_colums_features + 1
        
        tamura_coarseness_hasBeenCalculated = os.path.isfile('%s/features/tamura_coarseness.npy' % images_path)
        
        print 'Tamura coarseness features file found? ', tamura_coarseness_hasBeenCalculated
        
        if tamura_coarseness_hasBeenCalculated:
            tamura_coarseness_v = np.load('%s/features/tamura_coarseness.npy' % images_path)
        else : tamura_coarseness_v = np.zeros(25000)

    # Verifica se ja ha arquivo contendo os features de contraste e, caso houver,
    # carregue-o        
    if useTamuraContrast:
        
        n_colums_features = n_colums_features + 1
        
        tamura_contrast_hasBeenCalculated = os.path.isfile('%s/features/tamura_contrast.npy' % images_path)
        
        print 'Tamura contrast features file found? ', tamura_contrast_hasBeenCalculated
        
        if tamura_contrast_hasBeenCalculated:
            tamura_contrast_v = np.load('%s/features/tamura_contrast.npy' % images_path)
        else : tamura_contrast_v = np.zeros(25000)
    
    # Verifica se ja ha arquivo contendo os features de direcionalidade e, caso houver,
    # carregue-o
    if useTamuraDirectionality:
        
        n_colums_features = n_colums_features + 1
        
        tamura_directionality_hasBeenCalculated = os.path.isfile('%s/features/tamura_directionality.npy' % images_path)
        
        print 'Tamura directionality features file found? ', tamura_directionality_hasBeenCalculated
        
        if tamura_directionality_hasBeenCalculated:
            tamura_directionality_v = np.load('%s/features/tamura_directionality.npy' % images_path)
        else : tamura_directionality_v = np.zeros(25000)
    
    # Verifica se ja ha arquivo contendo os features de gabor e, caso houver,
    # carregue-o
    if useGarbor:
                
        # n_kernels = len(kernels)
        
        n_kernels = 16 # Ate que o problema do AdessoWiki da biblioteca  skimage seja resolvido.
        
        n_colums_features = n_colums_features + n_kernels * 2
        
        garbor_hasBeenCalculated = os.path.isfile('%s/features/garbor.npy' % images_path)
        
        print 'Gabor features file found? ', garbor_hasBeenCalculated
        
        if garbor_hasBeenCalculated:
            garbor_v = np.load('%s/features/garbor.npy' % images_path)
        else : garbor_v = np.zeros( 25000 * len(kernels) * 2)
        
    # Este trecho nao sera executado no adessoWiki, visto que ja geramos os arquivos pre-processados
    # externamente
    if __name__ == '__main__':
    
        r = 0;
    
        # Para cada uma das 25000 imagens
        for r in range (25000):
                
                img = None
                
                print 'Calculating histogram for file ', os.path.join(images_path, 'im%d.jpg' % (r + 1))
                
		# Se histogramas ja foram calculados, pula-se estaa etapa
                if not histogram_hasBeenCalculated:
                                    
                    if os.path.isfile(os.path.join(images_path, 'hist_%d.npy' % (r + 1))):
                        hist_RGB = np.load(os.path.join(images_path, 'hist_%d.npy' % (r + 1)))
                    else:
                        
                        if img is None:
                        
                            img = misc.imread(os.path.join(images_path, 'im%d.jpg' % (r + 1) ))
                    
                            if useGreyScale:
                                img = rgb2gray(img)
                        
                        
                        hist_RGB, bins = np.histogram(img, n_columns_histogram)
                        np.save(os.path.join(images_path, 'hist_%d.npy' % (r + 1)), hist_RGB)
                            
                    learning_set[r, :] = hist_RGB[:]

		# Os trechos a seguir verificam se cada uma das features ja foi calculada para a imagem r. Se sim, pulamos
                # a respectiva etapa
                           
                if useTamuraCoarseness:
                    
                    if not tamura_coarseness_hasBeenCalculated:
                        if os.path.isfile(os.path.join(images_path, 'tamura_coarseness_%d.npy' % (r + 1))):
                            tamura_coarseness_v[r] = np.load(os.path.join(images_path, 'tamura_coarseness_%d.npy' % (r + 1)))
                        else : 
                    
                            if img is None:
                        
                                img = misc.imread(os.path.join(images_path, 'im%d.jpg' % (r + 1) ))
                    
                                if useGreyScale:
                                    img = rgb2gray(img)
                    
                            print 'Calculating Tamura COARSENESS for file ', os.path.join(images_path, 'im%d.jpg' % (r + 1))
                            
                            try:
                                start_time = time.time()   
                                tamura_coarseness_v[r] = coarseness(img)
                                elapsed_time = time.time() - start_time
                                #print 'It took %ds to calculate COARSENESS...' % elapsed_time
                                np.save(os.path.join(images_path, 'tamura_coarseness_%d.npy' % (r + 1)),tamura_coarseness_v[r])
                                
                            except:
                                failures_coarseness.append(r+1)
                        
                        print tamura_coarseness_v[r] 
                        
                if useTamuraContrast:
                                                    
                    if not tamura_contrast_hasBeenCalculated:
                        
                        if os.path.isfile(os.path.join(images_path, 'tamura_contrast_%d.npy' % (r + 1))):
                            tamura_contrast_v[r] = np.load(os.path.join(images_path, 'tamura_contrast_%d.npy' % (r + 1)))
                        else : 
                            
                            if img is None:
                        
                                img = misc.imread(os.path.join(images_path, 'im%d.jpg' % (r + 1) ))
                    
                                if useGreyScale:
                                    img = rgb2gray(img)
                            
                            
                            print 'Calculating Tamura CONTRAST for file ', os.path.join(images_path, 'im%d.jpg' % (r + 1))
                            
                            try:
                                start_time = time.time()   
                                tamura_contrast_v[r] = contrast(img)
                                elapsed_time = time.time() - start_time
                                #print 'It took %ds to calculate CONTRAST...' % elapsed_time
                                np.save(os.path.join(images_path, 'tamura_contrast_%d.npy' % (r + 1)),tamura_contrast_v[r])
                            except:
                                failures_contrast.append(r+1)
                        
                        #print tamura_contrast_v[r]
                
                if useTamuraDirectionality:
                                                    
                    if not tamura_directionality_hasBeenCalculated:
                        
                        if os.path.isfile(os.path.join(images_path, 'tamura_directionality_%d.npy' % (r + 1))):
                            tamura_directionality_v[r] = np.load(os.path.join(images_path, 'tamura_directionality_%d.npy' % (r + 1)))
                        else : 
                            
                            if img is None:
                        
                                img = misc.imread(os.path.join(images_path, 'im%d.jpg' % (r + 1) ))
                    
                                if useGreyScale:
                                    img = rgb2gray(img)
                            
                            print 'Calculating Tamura DIRECTIONALITY for file ', os.path.join(images_path, 'im%d.jpg' % (r + 1))
                            
                            
                            try:                        
                                start_time = time.time()   
                                tamura_directionality_v[r] = degreeDirect(img, threshold, neigh)
                                elapsed_time = time.time() - start_time
                                np.save(os.path.join(images_path, 'tamura_directionality_%d.npy' % (r + 1)),tamura_directionality_v[r])
                                
                            except:
                                failures_directionality.append(r+1)
                                
                            
                            #print 'It took %ds to calculate DIRECTIONALITY...' % elapsed_time
                            
                            
                        
                        #print tamura_directionality_v[r]
                
                if useGarbor:
                    
                    start_i = len(kernels)*r * 2
                    stop_i = start_i + len(kernels) * 2
                    
                    if not garbor_hasBeenCalculated:
                        
                        if os.path.isfile(os.path.join(images_path, 'garbor_%d.npy' % (r + 1))):
                            garbor_v[ start_i : stop_i ] = np.load(os.path.join(images_path, 'garbor_%d.npy' % (r + 1)))
                        
                        else:
                            
                            if img is None:
                        
                                img = misc.imread(os.path.join(images_path, 'im%d.jpg' % (r + 1) ))
                    
                                if useGreyScale:
                                    img = rgb2gray(img)
                            
                            print 'Calculating GARBOR for file ', os.path.join(images_path, 'im%d.jpg' % (r + 1))
                            
                            try:
                                start_time = time.time()   
                                garbor_v[ start_i : stop_i ] = np.resize(garbor_features(img, kernels) , (1, len(kernels) * 2))
                                elapsed_time = time.time() - start_time
                                
                                #print 'It took %ds to calculate GARBOR...' % elapsed_time
                                
                                np.save(os.path.join(images_path, 'garbor_%d.npy' % (r + 1)), garbor_v[ start_i : stop_i ])
                                
                            except:
                                failures_garbor.append(r+1) 
        
	# Imprime indices das imagens que falharam por algum motivo. Os arquivos gerados externamente
	# contem dados para todas as imagens, isto eh, nao houve erros.
        print 'Failures COARSENESS ', failures_coarseness
        print 'Failures CONTRAST ', failures_contrast
        print 'Failures DIRECTIONALITY ', failures_directionality
        print 'Failures GARBOR ', failures_garbor   
        
	# Salva features, caso ainda nao estejam no sistema
        if not histogram_hasBeenCalculated:
            np.save('%s/features/histograms.npy' % images_path, learning_set)
            histogram_hasBeenCalculated = True
        
        if useTamuraCoarseness: 
            if not tamura_coarseness_hasBeenCalculated and not failures_coarseness:
                np.save('%s/features/tamura_coarseness.npy' % images_path, tamura_coarseness_v)
                tamura_coarseness_hasBeenCalculated = True
        
        if useTamuraContrast: 
            if not tamura_contrast_hasBeenCalculated and not failures_contrast:
                np.save('%s/features/tamura_contrast.npy' % images_path, tamura_contrast_v)
                tamura_contrast_hasBeenCalculated = True
        
        if useTamuraDirectionality: 
            if not tamura_directionality_hasBeenCalculated and not failures_directionality:
                np.save('%s/features/tamura_directionality.npy' % images_path, tamura_directionality_v)
                tamura_directionality_hasBeenCalculated = True
        
        if useGarbor: 
            if not garbor_hasBeenCalculated and not failures_garbor:
                np.save('%s/features/garbor.npy' % images_path, garbor_v)
                garbor_hasBeenCalculated = True
                
        print '%d images successfully preprocessed...' % (r+1)
        
        ##################################################################################################
        # Calculating 1o LEVEL K-means - Only features considered
        ##################################################################################################
        
        for cIter_features in [50, 70, 90, 100] :
        
            path_1 = "%s/arrays/level1_%d_%d" % (images_path, kCentroids_features, cIter_features)
        
            if not os.path.isdir(path_1):
                os.mkdir(path_1)
        
            print '1o level ',  kCentroids_features, cIter_features
        
            learning_set_features = np.zeros((n_images, n_colums_features))
            
	    # Constroi vetor de features para cada imagem
            for i in range(n_images):
                
                j = 0
                
                if useTamuraCoarseness:
                    learning_set_features[i, j] = tamura_coarseness_v[i]
                    j = j + 1
            
                if useTamuraContrast:
                    learning_set_features[i, j] = tamura_contrast_v[i]
                    j = j + 1
                    
                if useTamuraDirectionality:
                    learning_set_features[i, j] = tamura_directionality_v[i]
                    j = j + 1
                    
                if useGarbor:
                    
                    start_i = i * len(kernels) * 2
                    stop_i = start_i + len(kernels) * 2 
                    
                    learning_set_features[i, j : j + len(kernels) * 2] = garbor_v[ start_i : stop_i ]
                    j = j + len(kernels) * 2
            
            # A funcao spy.vq.kmeans requer que as colunas da matriz de observaoes possuam variancias unitarias.
            # Spy.vq.whiten normaliza as colunas, dividindo-as por uma unica constante (desvio padrao)
            whitenned_learning_set_features = spy.vq.whiten(learning_set_features)
            
            standard_deviations_features = np.zeros (n_colums_features)
            
            # Eh necessario calcular os desvios padroes e armazenados, ja que deverao ser utilizados tambem nas imagens procuradas
            for i in range(n_colums_features):
                standard_deviations_features[i] = learning_set_features[0, i] / whitenned_learning_set_features [0, i]
                
            # O metodo kmeans() gera os centroides e suas respectivas features. Isto eh, cada centroide eh uma 'imagem' com os features que mais 
            # descrevem o grupo.
            (centroids_codebook_features, distortion_features) = spy.vq.kmeans(whitenned_learning_set_features, kCentroids_features, cIter_features)
            
            # O metodo vq() retorna o centroide das imagens passadas como parametro, dado o resultado gerado por kmeans()
            (codes_features, dist_features) = spy.vq.vq(whitenned_learning_set_features, centroids_codebook_features)
            
            # Salva todos os resultados
            np.save('%s/centroids_codebook_features_%d_%d_%s_%s_%s_%s' % (path_1, kCentroids_features, cIter_features, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor), centroids_codebook_features)
            np.save('%s/standard_deviations_features_%d_%d_%s_%s_%s_%s' % (path_1, kCentroids_features, cIter_features, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor), standard_deviations_features)
            np.save('%s/vq_codes_obs_features_%d_%d_%s_%s_%s_%s' % (path_1, kCentroids_features, cIter_features, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor), codes_features)
            np.save('%s/vq_dist_features_%d_%d_%s_%s_%s_%s' % (path_1, kCentroids_features, cIter_features, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor), dist_features)
            
            ##################################################################################################
            # Calculating 2o LEVEL K-means - Histograms considered
            ##################################################################################################
            
            # Para cada centroide calculado anteriormente, calcula-se 20 outros centroides, baseando agora no histograma de nivel de cinza das imagens.
            # O processo de geracao de centroides eh identico ao caso anterior.
            for cIter in [50, 70, 90, 100] :
                  
                print '2o level ',  kCentroids, cIter
                  
                for i in range(kCentroids_features):
                    
                    path_2 = "%s/arrays/level2_%d_%d" % (images_path, kCentroids, cIter)
        
                    if not os.path.isdir(path_2):
                        os.mkdir(path_2)
                    
                    print 'Calculating 2o-level k-means for %d i 1o-level centroid' % i
                    
                    images_i = np.where(codes_features == i)[0]
                    
                    learning_set_histograms_i = learning_set[images_i, :]
                    
                    whitenned_learning_set_histograms_i = spy.vq.whiten(learning_set_histograms_i)
                
                    standard_deviations_histograms_i = np.zeros (n_columns_histogram)
                    
                    for j in range(n_columns_histogram):
                        standard_deviations_histograms_i[j] = learning_set_histograms_i[0, j] / whitenned_learning_set_histograms_i [0, j]
                        
                    (centroids_codebook_histograms_i, distortion_histograms_i) = spy.vq.kmeans(whitenned_learning_set_histograms_i, kCentroids, cIter)
                
                    (codes_histograms_i, dist_histograms_i) = spy.vq.vq(whitenned_learning_set_histograms_i, centroids_codebook_histograms_i)
                    
                    np.save('%s/centroids_codebook_images_index_%d_%d_%d_%s_%s_%s_%s' % (path_2, i , kCentroids, cIter, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor), images_i)
                    np.save('%s/centroids_codebook_histogram_%d_%d_%d_%s_%s_%s_%s' % (path_2, i , kCentroids, cIter, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor), centroids_codebook_histograms_i)
                    np.save('%s/standard_deviations_histogram_%d_%d_%d_%s_%s_%s_%s' % (path_2, i , kCentroids, cIter, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor), standard_deviations_histograms_i)
                    np.save('%s/vq_codes_obs_histogram_%d_%d_%d_%s_%s_%s_%s' % (path_2, i , kCentroids, cIter, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor), codes_histograms_i)
                    np.save('%s/vq_dist_histogram_%d_%d_%d_%s_%s_%s_%s' % (path_2, i , kCentroids, cIter, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor), dist_histograms_i)


============================
Busca de imagens semelhantes
============================

A busca de imagens semelhantes utiliza os vetores gerados pelo módulo anterior. 
Em poucas palavras, primeiramente, procura-se qual centróide do 1o nível corresponde melhor à imagem passada pelo cliente e, à partir de tal centróide, recupera-se as imagens da base mais 
próximas tendo em vista o resultado do 2o nível. Além disso, utilizamos também um algoritmo de Nearest Neighbors para visualizarmos os k vizinhos mais semelhantes internamente ao centróide. Para tal,
usamos o módulo sklearn.neighbors, disponível no servidor do AdessoWiki.

A fim de testarmos toda a aplicação, vamos procurar imagens que já  fazem parte da base de dados. Esperamos, portanto, que elas sejam recuperadas após a execução deste programa.
Os resultados serão comentados na próxima seção.

Assim como na seção anterior, comentários serão adicionados diretamente ao código do módulo.

Módulo find_images.py
=====================

.. code:: python
    :img_cols: 4
    :timeout: 200

    import numpy as np
    from PIL import Image
    import scipy.cluster as spy
    from scipy import misc
    import sklearn.neighbors as skn
    import os
    
    adread('rainbow.jpg')
    
    c_neighbors = 3
    
    image = '%s/im128.jpg' % images_path
    
    for image in ['%s/im128.jpg' % images_path, '%s/im24013.jpg' % images_path, '%s/im24799.jpg' % images_path, '/awmedia/www/media/Attachments/courseEA9791s2016/Projeto4/rainbow.jpg',  '/awmedia/www/media/Attachments/courseEA9791s2016/Projeto4/brasil.jpg']:
    
        # Leitura da imagem
        img = misc.imread(image)
                        
        if useGreyScale:
            img = rgb2gray(img)
        
        # Calculo do histograma 
        hist_RGB_given_image, bins = np.histogram(img, n_columns_histogram)
        
        path_1 = "%s/arrays/level1_%d_%d" % (images_path, kCentroids_features, cIter_features)
        
        # Recupera vetores calculados pelo KMEANS no pre-processamento
        centroids_codebook_features = np.load('%s/centroids_codebook_features_%d_%d_%s_%s_%s_%s.npy' % (path_1, kCentroids_features, cIter_features, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor))
        vq_codes_obs_features = np.load('%s/vq_codes_obs_features_%d_%d_%s_%s_%s_%s.npy' % (path_1, kCentroids_features, cIter_features, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor))
        standard_deviations_features = np.load('%s/standard_deviations_features_%d_%d_%s_%s_%s_%s.npy' % (path_1, kCentroids_features, cIter_features, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor))
        
        learning_set_features_image = np.zeros(n_colums_features)
        j = 0
        
        if useTamuraCoarseness:
            learning_set_features_image[j] = coarseness(img)
            j = j + 1
        
        if useTamuraContrast:
            learning_set_features_image[j] = contrast(img)
            j = j + 1
        
        if useTamuraDirectionality:
            learning_set_features_image[j] = degreeDirect(img, threshold, neigh)
            j = j + 1
        
        if useGarbor: 
            
            start_i = j
            stop_i = start_i + n_kernels * 2
            
            learning_set_features_image[start_i : stop_i] = np.resize(garbor_features(img, kernels) , (1, n_kernels * 2)) 
        
        for i in range (n_colums_features):
            learning_set_features_image[i] = learning_set_features_image[i] / standard_deviations_features[i]
        
        # Images from 1o level Kmeans
        (index, dist) = spy.vq.vq(np.array([learning_set_features_image]), centroids_codebook_features)
        
        path_2 = "%s/arrays/level2_%d_%d" % (images_path, kCentroids, cIter)
        
        # Recupera os centroides de 2o nivel relativo ao resultado do primeiro nivel
        images_index = np.load('%s/centroids_codebook_images_index_%d_%d_%d_%s_%s_%s_%s.npy' % (path_2, index[0] , kCentroids, cIter, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor))
        centroids_codebook_histogram = np.load('%s/centroids_codebook_histogram_%d_%d_%d_%s_%s_%s_%s.npy' % (path_2, index[0] , kCentroids, cIter, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor))
        vq_codes_obs_histogram = np.load('%s/vq_codes_obs_histogram_%d_%d_%d_%s_%s_%s_%s.npy' % (path_2, index[0] , kCentroids, cIter, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor))
        standard_deviations_histogram = np.load('%s/standard_deviations_histogram_%d_%d_%d_%s_%s_%s_%s.npy' % (path_2, index[0] , kCentroids, cIter, useTamuraCoarseness, useTamuraContrast, useTamuraDirectionality, useGarbor))
        
        # Normaliza histograma de acordo com os desvios padroes calculados pela funcao 
        # whitten() antes de realizar o KMEANS.  
        for i in range (n_columns_histogram):
            hist_RGB_given_image[i] = hist_RGB_given_image[i] / standard_deviations_histogram[i]
        
        # Calcula CENTROIDE mais proximo de 2o nivel
        (index, dist) = spy.vq.vq(np.array([hist_RGB_given_image]), centroids_codebook_histogram)
        
        # Recupera imagens que estao neste mesmo centroide de 2o nivel. Elimina-se imagens que estao nos 19 centroides de 2o nivel.
        same_centroid_images = np.where(vq_codes_obs_histogram == index )
        
        hist_RGB_dataset = []
        
        hist_RGB_all = np.load('%s/features/histograms.npy' % images_path)
        
        # Obtem-se histogramas das imagens deste mesmo centroide
        w = 0
        for i in same_centroid_images[0]:
            
            hist_RGB_dataset_i = hist_RGB_all[images_index[i] + 1, :]
                
            for j in range (n_columns_histogram):
                hist_RGB_dataset_i[j] = hist_RGB_dataset_i[j] / (standard_deviations_histogram[j] + 1e10)
            
            hist_RGB_dataset.append(hist_RGB_dataset_i)
            
            w = w + 1
        
        # Calcula-se as imagens mais proximos da imagem procurada DENTRO do centroide            
        nearest_neighbors = skn.NearestNeighbors(n_neighbors = c_neighbors).fit(hist_RGB_dataset)
        distances, indices = nearest_neighbors.kneighbors(hist_RGB_given_image)
                
        # Enfim, mostrar resultados
        
        img_original = adread(image)
        adshow(img_original, "Imagem original")
        
        j = 1
        
        for i in range(c_neighbors):
            
            img_index = images_index[same_centroid_images[0][indices[0][i]]]
            
            img = adread('%s/im%d.jpg' % (images_path, img_index +1))
            
            adshow(img, 'Vizinho no. %d' % j)
            
            j = j + 1
            
===========
Resultados
===========

Os resultados encontrados acima destacam alguns pontos distintos criados pelo nosso algoritmo. Nas três primeiras situações, pesquisamos imagens já presentes no banco de dados, enquanto que, na quarta, utilizamos duas figuras externas. Observa-se que, como esperado, nos 3 primeiros casos, as próprias figuras foram retornadas na pesquisa. 

A primeira pesquisa (pôr-do-sol) apresenta apenas um ponto brilhante bem definido com o restante predominantemente escuro. As imagens retornadas também apresentam tais características e, portanto, neste caso, o algoritmo foi bem sucedido. 

Em oposição, os casos 2 e 3 retornam imagens com features parecidas, mas com objetos muito distintos. No segundo, percebe-se que as texturas das plantas são muito parecidas com aquelas da imagem procurada. Por outro lado, as cores são distintas, resultado da escolha do grupo em utilizar os histogramas em níveis de cinza, já que preferimos um algoritmo que produzisse resultado mais rapidamente do que um que retornasse imagens mais semelhantes, devido a nossa grande base de dados.

A quarta figura, que não pertence ao banco de dados, também aproximou-se dos casos 2 e 3. O vizinho no. 1 assemelha-se à imagem procurada em termos de vegetação: tanto ela quanto a original possuem árvores e plantas. Da mesma forma, o vizinho 3 contém uma grande aŕea onde o céu é predominante, assim como a original. 

Enfim, a quinta figura apresentou resultados muito bons, uma vez que duas dos três vizinhos também são construções do mesmo estilo.

============
Conclusões
============

De maneira geral, pudemos desenvolver os conceitos trabalhados durante a disciplina, principalmente a primeira metade, em que discutimos a filtragem de imagens, de forma prática e em um problema que detém a atenção de grandes empresas atualmente, como a Google, por exemplo. Em relação aos resultados encontrados,  julgamos termos atendido o objetivo inicial, mesmo que, para algumas imagens, o resultado seja um pouco distinto daquele procurado. Vale lembrar que não desenvolvemos nenhum algoritmo de visão computacional, isto é, não buscamos interpretar a imagem, mas sim, só obter características de cor e textura.

Encontramos também um problema que não pôde ser resolvido, devido a pequena quantidade de tempo e a alta quantidade de imagens: a biblioteca PIL do AdessoWiki está numa versão anterior à 12, que foi utilizada no pré-processamento das imagens. Sendo assim, utilizamos dados calculados em outra máquina para procurar imagens no servidor do AdessoWiki e, dessa forma, encontramos algumas diferenças.

Enfim, o projeto foi uma oportunidade muito válida de concretizar e encerrar o curso de Processamento de Imagens.

============
Referências
============

[1] Prof. Georgy Gimel'farb. Lecture Notes from the course "Hypermedia and Multimedia Systems" at the University of Auckland.
"CBIR: Texture Features". Disponível em:
https://www.cs.auckland.ac.nz/courses/compsci708s1c/lectures/Glect-html/topic4c708FSC.htm#tamura

[2] Wikipédia - Gabor Filter. Disponível em:
http://en.wikipedia.org/wiki/Gabor_filter

[3] http://mplab.ucsd.edu/tutorials/gabor.pdf

